<!doctype html><html lang=ko dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[혼공머신] 3주차 학습 내용 정리 | Halog</title>
<meta name=keywords content=","><meta name=description content="혼자 공부하는 머신러닝+딥러닝 3주차 학습 내용"><meta name=author content><link rel=canonical href=https://haservi.github.io/posts/books/hg-mldl/week-3/><meta name=google-site-verification content="G-MXZP81P04W"><link rel=stylesheet as=style crossorigin href=https://cdn.jsdelivr.net/gh/orioncactus/pretendard@v1.3.6/dist/web/static/pretendard.css><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://haservi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://haservi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://haservi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://haservi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://haservi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=ko href=https://haservi.github.io/posts/books/hg-mldl/week-3/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1400973749140762" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-RP5NDCM8J9"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-RP5NDCM8J9")}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://haservi.github.io/ accesskey=h title="Halog (Alt + H)">Halog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://haservi.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://haservi.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://haservi.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://haservi.github.io/>홈</a>&nbsp;»&nbsp;<a href=https://haservi.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">[혼공머신] 3주차 학습 내용 정리</h1><div class=post-meta><span title='2025-07-17 22:42:45 +0900 +0900'>7월 17, 2025</span>&nbsp;·&nbsp;9 분</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>목차</span></summary><div class=inner><ul><li><a href=#%eb%8a%90%eb%82%80-%ec%a0%90 aria-label="느낀 점">느낀 점</a></li><li><a href=#%ea%b8%b0%eb%b3%b8-%ec%88%99%ec%a0%9c%ed%95%84%ec%88%98 aria-label="기본 숙제(필수)">기본 숙제(필수)</a></li><li><a href=#%eb%82%b4%ec%9a%a9-%ec%a0%95%eb%a6%ac aria-label="내용 정리">내용 정리</a><ul><li><a href=#04-1-%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1-%ed%9a%8c%ea%b7%80 aria-label="04-1 로지스틱 회귀">04-1 로지스틱 회귀</a><ul><li><a href=#%eb%9f%ad%ed%82%a4%eb%b0%b1%ec%9d%98-%ed%99%95%eb%a5%a0 aria-label="럭키백의 확률">럭키백의 확률</a></li><li><a href=#%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1-%ed%9a%8c%ea%b7%80 aria-label="로지스틱 회귀">로지스틱 회귀</a></li><li><a href=#%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1-%ed%9a%8c%ea%b7%80%eb%a1%9c-%eb%8b%a4%ec%a4%91-%eb%b6%84%eb%a5%98-%ec%88%98%ed%96%89%ed%95%98%ea%b8%b0 aria-label="로지스틱 회귀로 다중 분류 수행하기">로지스틱 회귀로 다중 분류 수행하기</a></li></ul></li><li><a href=#04-2-%ed%99%95%eb%a5%a0%ec%a0%81-%ea%b2%bd%ec%82%ac-%ed%95%98%ea%b0%95%eb%b2%95 aria-label="04-2 확률적 경사 하강법">04-2 확률적 경사 하강법</a><ul><li><a href=#%ec%a0%90%ec%a7%84%ec%a0%81%ec%9d%b8-%ed%95%99%ec%8a%b5 aria-label="점진적인 학습">점진적인 학습</a></li><li><a href=#%ed%99%95%eb%a5%a0%ec%a0%81-%ea%b2%bd%ec%82%ac-%ed%95%98%ea%b0%95%eb%b2%95 aria-label="확률적 경사 하강법">확률적 경사 하강법</a></li><li><a href=#%ec%86%90%ec%8b%a4-%ed%95%a8%ec%88%98 aria-label="손실 함수">손실 함수</a></li><li><a href=#%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1-%ec%86%90%ec%8b%a4-%ed%95%a8%ec%88%98 aria-label="로지스틱 손실 함수">로지스틱 손실 함수</a></li><li><a href=#sgdclassifier aria-label=SGDClassifier>SGDClassifier</a></li><li><a href=#%ec%97%90%ed%8f%ac%ed%81%ac%ec%99%80-%ea%b3%bc%eb%8c%80%ea%b3%bc%ec%86%8c%ec%a0%81%ed%95%a9 aria-label="에포크와 과대/과소적합">에포크와 과대/과소적합</a></li><li><a href=#%ec%a0%90%ec%a7%84%ec%a0%81-%ed%95%99%ec%8a%b5%ec%9d%84-%ec%9c%84%ed%95%9c-%ed%99%95%eb%a5%a0%ec%a0%81-%ea%b2%bd%ec%82%ac-%ed%95%98%ea%b0%95%eb%b2%95 aria-label="점진적 학습을 위한 확률적 경사 하강법">점진적 학습을 위한 확률적 경사 하강법</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><p><img alt=image loading=lazy src=/posts/books/hg-mldl/week-3/images/image-01.webp></p><p>벌써 3주차를 하게 되네요. 하면서 느낀 점은 이거를 실무에서 쓰려면 공부를 많이 많이.. 해야 할 것 같아요.</p><p>이틀 전에 공부한 회귀 중에 릿지? 랏쏘? 이런게 있는거 같은데 벌써 까먹은거면 보면..</p><p>차라리 <a href=https://huggingface.co/>허깅페이스</a>를 통해서 이미 학습된 모델을 잘 이용하는게 좋을 것 같다는 생각도 듭니다..</p><p>딥러닝이 궁금해서 하긴 하는데 차트며.. 공식이며.. 생각보다 어렵고 이거를 이해하고 활용해서 내가 필요에 의해 쓸 수 있을까?? 라는 생각이 드네요.</p><p>이 시간에 알고리즘이나 AWS, 아키텍처 같은 공부를 하는게 더 낫지 않을까 라는 생각도 들지만..</p><p>뭐.. 그래도 잘 따라가면 얻는게 없지는 않을 것 같습니다. 조금이라도 아는게 있으면 조금 더 관심이 갈 수 있으니까요. 아마..?</p><p>깊게 고민하기 보다는 가볍게 실습을 위주로 하며, 아~ 그렇구나.. 하는 생각으로 가볍게 읽어봐야겠어요.</p><h2 id=느낀-점>느낀 점<a hidden class=anchor aria-hidden=true href=#느낀-점>#</a></h2><p>로지스틱 회귀는 쓸 데가 있어 보입니다.. 아래 내용 정리를 하면서 사용자 특성(성별, 나이, 이용 시간)을 활용해 웹 사이트의 또는 유튜브 광고 클릭율을
예측하는데 쓰이는 것 같아요.</p><p>실제로 넷플릭스, 아마존, 구글 같은 회사들이 사용자 정보를 바탕으로 사용자 행동을 예측하고 개인화 서비스를 제공한다고 합니다.</p><p>그렇다고 합니다.</p><p>추가로 알고리즘 팀과 협업할 때 아리송한 언어들을 공부하니까 조금 도움이 되는 것 같습니다. :D..</p><h2 id=기본-숙제필수>기본 숙제(필수)<a hidden class=anchor aria-hidden=true href=#기본-숙제필수>#</a></h2><p>Ch.04(04-1) 2번 문제 풀고, 풀이 과정 설명하기</p><p>문제: 로지스틱 회귀가 이진 분류에서 확률을 출력하기 위해 사용하는 함수는?</p><p>시그모이드 함수, 소프트맥스 함수</p><p>시그모이드 함수는 이진 분류에서 사용하며, 소프트맥스 함수는 다중 분류에서 사용합니다.</p><p>일반적으로 클래스 2개인 경우는 시그모이드, 클래스가 3개 이상인 경우는 소프트맥스 함수를 사용하는게 좋습니다.</p><p>소프트 맥스는 1에 수렴하는 확률을 표시하기 때문에 사용자 입장에서 더 이해하기 쉽습니다.</p><h2 id=내용-정리>내용 정리<a hidden class=anchor aria-hidden=true href=#내용-정리>#</a></h2><h3 id=04-1-로지스틱-회귀>04-1 로지스틱 회귀<a hidden class=anchor aria-hidden=true href=#04-1-로지스틱-회귀>#</a></h3><p>로지텍 럭키 박스처럼 상자안에 확률적으로 도미 또는 빙어가 들어가 있습니다.</p><h4 id=럭키백의-확률>럭키백의 확률<a hidden class=anchor aria-hidden=true href=#럭키백의-확률>#</a></h4><p>길이, 높이, 두께, 대각성 길이, 무게를 사용하여 도미 또는 빙어를 맞춰봅니다.</p><p>k-최근접 이웃의 확률을 통해서 위의 데이터로 맞춰봅니다. 먼저 샘플 데이터를 가져옵니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fish</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;https://bit.ly/fish_csv_data&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>fish</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><table><thead><tr><th>index</th><th>Species</th><th>Weight</th><th>Length</th><th>Diagonal</th><th>Height</th><th>Width</th></tr></thead><tbody><tr><td>0</td><td>Bream</td><td>242.0</td><td>25.4</td><td>30.0</td><td>11.52</td><td>4.02</td></tr><tr><td>1</td><td>Bream</td><td>290.0</td><td>26.3</td><td>31.2</td><td>12.48</td><td>4.3056</td></tr><tr><td>2</td><td>Bream</td><td>340.0</td><td>26.5</td><td>31.1</td><td>12.3778</td><td>4.6961</td></tr><tr><td>3</td><td>Bream</td><td>363.0</td><td>29.0</td><td>33.5</td><td>12.73</td><td>4.4555</td></tr><tr><td>4</td><td>Bream</td><td>430.0</td><td>29.0</td><td>34.0</td><td>12.444</td><td>5.134</td></tr></tbody></table><p>판다스를 이용해 샘플 데이터를 읽어옵니다. 위와 같이 샘플 데이터를 볼 수 있습니다. 실제로는 해당 링크에 가면 더 많은 데이터를 볼 수 있습니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>pd</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>fish</span><span class=p>[</span><span class=s1>&#39;Species&#39;</span><span class=p>]))</span>
</span></span></code></pre></td></tr></table></div></div><p>실제로 스페셜 열의 유니크 값을 출력해보면 [&lsquo;Bream&rsquo; &lsquo;Roach&rsquo; &lsquo;Whitefish&rsquo; &lsquo;Parkki&rsquo; &lsquo;Perch&rsquo; &lsquo;Pike&rsquo; &lsquo;Smelt&rsquo;] 이라는 항목들이 있는 것을 볼 수 있습니다.</p><p><code>to_numpy()</code> 메서드로 넘파이 배열로 바꾸어 fish_input에 5개의 특성을 나오도록 입력 데이터를 생성합니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>fish_input</span> <span class=o>=</span> <span class=n>fish</span><span class=p>[[</span><span class=s1>&#39;Weight&#39;</span><span class=p>,</span><span class=s1>&#39;Length&#39;</span><span class=p>,</span><span class=s1>&#39;Diagonal&#39;</span><span class=p>,</span><span class=s1>&#39;Height&#39;</span><span class=p>,</span><span class=s1>&#39;Width&#39;</span><span class=p>]]</span><span class=o>.</span><span class=n>to_numpy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>fish_input</span><span class=p>[:</span><span class=mi>10</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>[[</span><span class=mf>242.</span>      <span class=mf>25.4</span>     <span class=mf>30.</span>      <span class=mf>11.52</span>     <span class=mf>4.02</span>  <span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>290.</span>      <span class=mf>26.3</span>     <span class=mf>31.2</span>     <span class=mf>12.48</span>     <span class=mf>4.3056</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>340.</span>      <span class=mf>26.5</span>     <span class=mf>31.1</span>     <span class=mf>12.3778</span>   <span class=mf>4.6961</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>363.</span>      <span class=mf>29.</span>      <span class=mf>33.5</span>     <span class=mf>12.73</span>     <span class=mf>4.4555</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>430.</span>      <span class=mf>29.</span>      <span class=mf>34.</span>      <span class=mf>12.444</span>    <span class=mf>5.134</span> <span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>450.</span>      <span class=mf>29.7</span>     <span class=mf>34.7</span>     <span class=mf>13.6024</span>   <span class=mf>4.9274</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>500.</span>      <span class=mf>29.7</span>     <span class=mf>34.5</span>     <span class=mf>14.1795</span>   <span class=mf>5.2785</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>390.</span>      <span class=mf>30.</span>      <span class=mf>35.</span>      <span class=mf>12.67</span>     <span class=mf>4.69</span>  <span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>450.</span>      <span class=mf>30.</span>      <span class=mf>35.1</span>     <span class=mf>14.0049</span>   <span class=mf>4.8438</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>500.</span>      <span class=mf>30.7</span>     <span class=mf>36.2</span>     <span class=mf>14.2266</span>   <span class=mf>4.9594</span><span class=p>]]</span>
</span></span></code></pre></td></tr></table></div></div><p>필요한 데이터를 준비합니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>fish_input</span> <span class=o>=</span> <span class=n>fish</span><span class=p>[[</span><span class=s1>&#39;Weight&#39;</span><span class=p>,</span><span class=s1>&#39;Length&#39;</span><span class=p>,</span><span class=s1>&#39;Diagonal&#39;</span><span class=p>,</span><span class=s1>&#39;Height&#39;</span><span class=p>,</span><span class=s1>&#39;Width&#39;</span><span class=p>]]</span><span class=o>.</span><span class=n>to_numpy</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>fish_target</span> <span class=o>=</span> <span class=n>fish</span><span class=p>[</span><span class=s1>&#39;Species&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>to_numpy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 훈련 세트와 테스트 세트로 나누기</span>
</span></span><span class=line><span class=cl><span class=n>train_input</span><span class=p>,</span> <span class=n>test_input</span><span class=p>,</span> <span class=n>train_target</span><span class=p>,</span> <span class=n>test_target</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>fish_input</span><span class=p>,</span> <span class=n>fish_target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># StandardScaler 클래스를 사용해 훈련 세트와 테스트 세트를 표준화 전처리</span>
</span></span><span class=line><span class=cl><span class=n>ss</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>ss</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_input</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>train_scaled</span> <span class=o>=</span> <span class=n>ss</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>train_input</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>test_scaled</span> <span class=o>=</span> <span class=n>ss</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_input</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>위의 데이터로 훈련 세트와 테스트 세트의 점수를 확인합니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.neighbors</span> <span class=kn>import</span> <span class=n>KNeighborsClassifier</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>kn</span> <span class=o>=</span> <span class=n>KNeighborsClassifier</span><span class=p>(</span><span class=n>n_neighbors</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>kn</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_scaled</span><span class=p>,</span> <span class=n>train_target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>kn</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>train_scaled</span><span class=p>,</span> <span class=n>train_target</span><span class=p>))</span> <span class=c1># 0.8907563025210085</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>kn</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>test_scaled</span><span class=p>,</span> <span class=n>test_target</span><span class=p>))</span> <span class=c1># 0.85</span>
</span></span></code></pre></td></tr></table></div></div><p>기존의 Species 의 타깃 데이터에 2개 이상의 클래스가 포함된 문제를 <strong>다중 분류</strong>라고 부릅니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>proba</span> <span class=o>=</span> <span class=n>kn</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>test_scaled</span><span class=p>[:</span><span class=mi>5</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>proba</span><span class=p>,</span> <span class=n>decimals</span><span class=o>=</span><span class=mi>4</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>[[</span><span class=mf>0.</span>     <span class=mf>0.</span>     <span class=mf>1.</span>     <span class=mf>0.</span>     <span class=mf>0.</span>     <span class=mf>0.</span>     <span class=mf>0.</span>    <span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.</span>     <span class=mf>0.</span>     <span class=mf>0.</span>     <span class=mf>0.</span>     <span class=mf>0.</span>     <span class=mf>1.</span>     <span class=mf>0.</span>    <span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.</span>     <span class=mf>0.</span>     <span class=mf>0.</span>     <span class=mf>1.</span>     <span class=mf>0.</span>     <span class=mf>0.</span>     <span class=mf>0.</span>    <span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.</span>     <span class=mf>0.</span>     <span class=mf>0.6667</span> <span class=mf>0.</span>     <span class=mf>0.3333</span> <span class=mf>0.</span>     <span class=mf>0.</span>    <span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.</span>     <span class=mf>0.</span>     <span class=mf>0.6667</span> <span class=mf>0.</span>     <span class=mf>0.3333</span> <span class=mf>0.</span>     <span class=mf>0.</span>    <span class=p>]]</span>
</span></span></code></pre></td></tr></table></div></div><p>위와 같이 <code>predict_proba</code> 메서드를 이용하여 classes_ 속성과 동일하게 알파벳 순서로 타깃의 확률에 대한 정보를 확인할 수 있습니다.</p><h4 id=로지스틱-회귀>로지스틱 회귀<a hidden class=anchor aria-hidden=true href=#로지스틱-회귀>#</a></h4><p>로지스틱 회귀는 이름은 회귀이지만 분류 모델입니다.</p><p>핵심 개념</p><ul><li>선형 회귀와 달리 0과 1 사이의 확률값을 출력</li><li>시그모이드 함수를 사용해서 선형 결합을 확률로 변환</li><li>주로 이진 분류(예: 스팸/정상 메일, 합격/불합격)에 사용</li></ul><p>웹 서비스의 클릭률 예측, 의료 진단, 마케팅 타겟팅 등에 널리 사용된다고 합니다..</p><p>시그모이드 그래프를 그려보면 아래와 같습니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=o>-</span><span class=mi>5</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>phi</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>z</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>z</span><span class=p>,</span> <span class=n>phi</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;z&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;phi&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p><img alt=image loading=lazy src=/posts/books/hg-mldl/week-3/images/image-02.webp></p><p>로지스틱 회귀로 이진 분류 수행하기 위해서는 불리언 인덱싱을 이용하여 판단할 수 있습니다.</p><p>아래와 같이 True인 항목만 출력되는 것을 확인할 수 있습니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>char_arr</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=s1>&#39;A&#39;</span><span class=p>,</span> <span class=s1>&#39;B&#39;</span><span class=p>,</span> <span class=s1>&#39;C&#39;</span><span class=p>,</span> <span class=s1>&#39;D&#39;</span><span class=p>,</span> <span class=s1>&#39;E&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>char_arr</span><span class=p>[[</span><span class=kc>True</span><span class=p>,</span> <span class=kc>False</span><span class=p>,</span> <span class=kc>True</span><span class=p>,</span> <span class=kc>False</span><span class=p>,</span> <span class=kc>False</span><span class=p>]])</span> <span class=c1># [&#39;A&#39; &#39;C&#39;]</span>
</span></span></code></pre></td></tr></table></div></div><p>위와 같이 도미(Bream)와 빙어(Smelt) 행만 골라서 도미와 빙어 데이터를 고릅니다.</p><p>이를 이용해 로지스틱 회귀를 이용해 샘플데이터가 빙어인지 도미인지 확인해봅니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>bream_smelt_indexes</span> <span class=o>=</span> <span class=p>(</span><span class=n>train_target</span> <span class=o>==</span> <span class=s1>&#39;Bream&#39;</span><span class=p>)</span> <span class=o>|</span> <span class=p>(</span><span class=n>train_target</span> <span class=o>==</span> <span class=s1>&#39;Smelt&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>train_bream_smelt</span> <span class=o>=</span> <span class=n>train_scaled</span><span class=p>[</span><span class=n>bream_smelt_indexes</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>target_bream_smelt</span> <span class=o>=</span> <span class=n>train_target</span><span class=p>[</span><span class=n>bream_smelt_indexes</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LogisticRegression</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>lr</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>lr</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_bream_smelt</span><span class=p>,</span> <span class=n>target_bream_smelt</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 5개의 샘플의 도미 빙어 판단</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>lr</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>train_bream_smelt</span><span class=p>[:</span><span class=mi>5</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=c1># [&#39;Bream&#39; &#39;Smelt&#39; &#39;Bream&#39; &#39;Bream&#39; &#39;Bream&#39;]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 5개의 샘플의 확률 판단(알파벳 순서이므로 도미가 먼저)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>lr</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>train_bream_smelt</span><span class=p>[:</span><span class=mi>5</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=p>[[</span><span class=mf>0.99760007</span> <span class=mf>0.00239993</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.02737325</span> <span class=mf>0.97262675</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.99486386</span> <span class=mf>0.00513614</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.98585047</span> <span class=mf>0.01414953</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.99767419</span> <span class=mf>0.00232581</span><span class=p>]]</span>
</span></span></code></pre></td></tr></table></div></div><p>위와 같이 샘플 5개 중 4개가 도미로 예측했습니다.</p><p>아래는 로지스틱 회귀가 학습한 계수를 확인할 수 있습니다.</p><p>공식같은게 있는데.. 이걸 돌리면 됩니다. 아래 주석을 참조하여 로지스틱 회귀의 작동 원리??(점점 멘붕..)를 살펴봅니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>lr</span><span class=o>.</span><span class=n>coef_</span><span class=p>,</span> <span class=n>lr</span><span class=o>.</span><span class=n>intercept_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># [[-0.40451732 -0.57582787 -0.66248158 -1.01329614 -0.73123131]] [-2.16172774]</span>
</span></span><span class=line><span class=cl><span class=c1># 위 각 속성에 맞게 곱한 다음.. 마지막에 마지막 꺼 빼기..를 하는게 아래의 decision_function 함수의 역할..!</span>
</span></span><span class=line><span class=cl><span class=n>decisions</span> <span class=o>=</span> <span class=n>lr</span><span class=o>.</span><span class=n>decision_function</span><span class=p>(</span><span class=n>train_bream_smelt</span><span class=p>[:</span><span class=mi>5</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>decisions</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># [-6.02991358  3.57043428 -5.26630496 -4.24382314 -6.06135688]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 위의 값(z)을 시그모이드 함수? 아래 라이브러리를 통해 expit하면 시그모이드 값을 알 수 있음..</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>scipy.special</span> <span class=kn>import</span> <span class=n>expit</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>expit</span><span class=p>(</span><span class=n>decisions</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># [0.00239993 0.97262675 0.00513614 0.01414953 0.00232581]</span>
</span></span></code></pre></td></tr></table></div></div><p>결론만 놓고 보면 위의 expit을 통해서 가장 위의 5개의 샘플 중에 도미는 0에 가까워지고 빙어는 1에 가까워집니다.</p><h4 id=로지스틱-회귀로-다중-분류-수행하기>로지스틱 회귀로 다중 분류 수행하기<a hidden class=anchor aria-hidden=true href=#로지스틱-회귀로-다중-분류-수행하기>#</a></h4><p>LogisticRegression 클래스는 기본적으로 반복적인 알고리즘을 사용합니다. max_iter 매개변수에서 반복 횟수를 지정(기본 100)합니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>lr</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>(</span><span class=n>C</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>1000</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>lr</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_scaled</span><span class=p>,</span> <span class=n>train_target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>lr</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>train_scaled</span><span class=p>,</span> <span class=n>train_target</span><span class=p>))</span> <span class=c1># 0.9327731092436975</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>lr</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>test_scaled</span><span class=p>,</span> <span class=n>test_target</span><span class=p>))</span> <span class=c1># 0.925</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>lr</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>test_scaled</span><span class=p>[:</span><span class=mi>5</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=c1># [&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Perch&#39;]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>proba</span> <span class=o>=</span> <span class=n>lr</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>test_scaled</span><span class=p>[:</span><span class=mi>5</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>proba</span><span class=p>,</span> <span class=n>decimals</span><span class=o>=</span><span class=mi>3</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>lr</span><span class=o>.</span><span class=n>classes_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 5개의 샘플에 대해 7개의 물고기? 종의 확률 값</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=s1>&#39;Bream&#39;</span> <span class=s1>&#39;Parkki&#39;</span> <span class=s1>&#39;Perch&#39;</span> <span class=s1>&#39;Pike&#39;</span> <span class=s1>&#39;Roach&#39;</span> <span class=s1>&#39;Smelt&#39;</span> <span class=s1>&#39;Whitefish&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>[[</span><span class=mf>0.</span>    <span class=mf>0.014</span> <span class=mf>0.842</span> <span class=mf>0.</span>    <span class=mf>0.135</span> <span class=mf>0.007</span> <span class=mf>0.003</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.</span>    <span class=mf>0.003</span> <span class=mf>0.044</span> <span class=mf>0.</span>    <span class=mf>0.007</span> <span class=mf>0.946</span> <span class=mf>0.</span>   <span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.</span>    <span class=mf>0.</span>    <span class=mf>0.034</span> <span class=mf>0.934</span> <span class=mf>0.015</span> <span class=mf>0.016</span> <span class=mf>0.</span>   <span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.011</span> <span class=mf>0.034</span> <span class=mf>0.305</span> <span class=mf>0.006</span> <span class=mf>0.567</span> <span class=mf>0.</span>    <span class=mf>0.076</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.</span>    <span class=mf>0.</span>    <span class=mf>0.904</span> <span class=mf>0.002</span> <span class=mf>0.089</span> <span class=mf>0.002</span> <span class=mf>0.001</span><span class=p>]]</span>
</span></span></code></pre></td></tr></table></div></div><p>소프트맥스 함수: 여러 개의 선형 방적식의 출력값을 0~1 사이로 압축하고 전체 합이 1이 되도록 만듬. 이를 위해 지수 함수를 사용하기 때문에
정규화된 지수 함수라고도 부름</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>decision</span> <span class=o>=</span> <span class=n>lr</span><span class=o>.</span><span class=n>decision_function</span><span class=p>(</span><span class=n>test_scaled</span><span class=p>[:</span><span class=mi>5</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>decision</span><span class=p>,</span> <span class=n>decimals</span><span class=o>=</span><span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>[[</span> <span class=o>-</span><span class=mf>6.51</span>   <span class=mf>1.04</span>   <span class=mf>5.17</span>  <span class=o>-</span><span class=mf>2.76</span>   <span class=mf>3.34</span>   <span class=mf>0.35</span>  <span class=o>-</span><span class=mf>0.63</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=o>-</span><span class=mf>10.88</span>   <span class=mf>1.94</span>   <span class=mf>4.78</span>  <span class=o>-</span><span class=mf>2.42</span>   <span class=mf>2.99</span>   <span class=mf>7.84</span>  <span class=o>-</span><span class=mf>4.25</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span> <span class=o>-</span><span class=mf>4.34</span>  <span class=o>-</span><span class=mf>6.24</span>   <span class=mf>3.17</span>   <span class=mf>6.48</span>   <span class=mf>2.36</span>   <span class=mf>2.43</span>  <span class=o>-</span><span class=mf>3.87</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span> <span class=o>-</span><span class=mf>0.69</span>   <span class=mf>0.45</span>   <span class=mf>2.64</span>  <span class=o>-</span><span class=mf>1.21</span>   <span class=mf>3.26</span>  <span class=o>-</span><span class=mf>5.7</span>    <span class=mf>1.26</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span> <span class=o>-</span><span class=mf>6.4</span>   <span class=o>-</span><span class=mf>1.99</span>   <span class=mf>5.82</span>  <span class=o>-</span><span class=mf>0.13</span>   <span class=mf>3.5</span>   <span class=o>-</span><span class=mf>0.09</span>  <span class=o>-</span><span class=mf>0.7</span> <span class=p>]]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>scipy.special</span> <span class=kn>import</span> <span class=n>softmax</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>proba</span> <span class=o>=</span> <span class=n>softmax</span><span class=p>(</span><span class=n>decision</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>proba</span><span class=p>,</span> <span class=n>decimals</span><span class=o>=</span><span class=mi>3</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 행의 값을 더하면 1에 수렴..</span>
</span></span><span class=line><span class=cl><span class=p>[[</span><span class=mf>0.</span>    <span class=mf>0.014</span> <span class=mf>0.842</span> <span class=mf>0.</span>    <span class=mf>0.135</span> <span class=mf>0.007</span> <span class=mf>0.003</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.</span>    <span class=mf>0.003</span> <span class=mf>0.044</span> <span class=mf>0.</span>    <span class=mf>0.007</span> <span class=mf>0.946</span> <span class=mf>0.</span>   <span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.</span>    <span class=mf>0.</span>    <span class=mf>0.034</span> <span class=mf>0.934</span> <span class=mf>0.015</span> <span class=mf>0.016</span> <span class=mf>0.</span>   <span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.011</span> <span class=mf>0.034</span> <span class=mf>0.305</span> <span class=mf>0.006</span> <span class=mf>0.567</span> <span class=mf>0.</span>    <span class=mf>0.076</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.</span>    <span class=mf>0.</span>    <span class=mf>0.904</span> <span class=mf>0.002</span> <span class=mf>0.089</span> <span class=mf>0.002</span> <span class=mf>0.001</span><span class=p>]]</span>
</span></span></code></pre></td></tr></table></div></div><p>로지스틱 회귀는 값을 0 ~ 1 사이로 압축하기 때문에 우리는 이 값을 0 ~100% 사이로 이해할 수 있습니다.</p><h3 id=04-2-확률적-경사-하강법>04-2 확률적 경사 하강법<a hidden class=anchor aria-hidden=true href=#04-2-확률적-경사-하강법>#</a></h3><h4 id=점진적인-학습>점진적인 학습<a hidden class=anchor aria-hidden=true href=#점진적인-학습>#</a></h4><p>훈련한 모델을 버리지 않고 조금 씩 훈련을 쌓는 방식을 점진적인 학습이라고 부릅니다.</p><p>대표적으로 점진적 학습 알고리즘은 <strong>확률적 경사 하강법</strong> 입니다.</p><h4 id=확률적-경사-하강법>확률적 경사 하강법<a hidden class=anchor aria-hidden=true href=#확률적-경사-하강법>#</a></h4><p>확률적 경사 하강법(Stochastic Gradient Descent, SGD)은 머신러닝에서 모델을 학습시킬 때 사용하는 최적화 알고리즘입니다.</p><p>기본 개념</p><p>일반적인 경사 하강법은 전체 데이터셋을 사용해 기울기를 계산하지만, SGD는 한 번에 하나의 데이터 포인트만 사용해서 기울기를 계산하고 가중치를 업데이트합니다.</p><p>위와 같이 한 번 모두 사용하는 과정을 <strong>에포크</strong> 라고 부릅니다.</p><p><img alt=image loading=lazy src=/posts/books/hg-mldl/week-3/images/image-03.png></p><p>위와 같은 그림으로 이해하면 좋을 것 같습니다.</p><h4 id=손실-함수>손실 함수<a hidden class=anchor aria-hidden=true href=#손실-함수>#</a></h4><p><strong>손실 함수</strong>는 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준입니다.</p><h4 id=로지스틱-손실-함수>로지스틱 손실 함수<a hidden class=anchor aria-hidden=true href=#로지스틱-손실-함수>#</a></h4><p>손실 함수</p><p>손실 함수는 예측과 정답(타깃)을 곱한 다음 음수로 바꿔서 계산합니다.</p><p>로그 함수는 0에 가까울수록 아주 큰 음수가 되기 때문에 손실을 아주 크게 만들어 모델에 큰 영향을 줍니다.</p><p>이진 분류는 <strong>로지스틱 손실 함수</strong>를 사용하고 다중 분류는 <strong>크로스엔트로피 손실 함수</strong>를 사용합니다.</p><h4 id=sgdclassifier>SGDClassifier<a hidden class=anchor aria-hidden=true href=#sgdclassifier>#</a></h4><p>SGDClassifier의 객체를 만들 때 2개의 매개변수를 지정합니다.</p><p>loss는 손실 함수의 종류를 지정합니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fish</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;https://bit.ly/fish_csv_data&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 종(물고기) 종류를 제외한 나머지 5개는 입력 데이터로 사용 종(물고기) 열은 타깃 데이터</span>
</span></span><span class=line><span class=cl><span class=n>fish_input</span> <span class=o>=</span> <span class=n>fish</span><span class=p>[[</span><span class=s1>&#39;Weight&#39;</span><span class=p>,</span><span class=s1>&#39;Length&#39;</span><span class=p>,</span><span class=s1>&#39;Diagonal&#39;</span><span class=p>,</span><span class=s1>&#39;Height&#39;</span><span class=p>,</span><span class=s1>&#39;Width&#39;</span><span class=p>]]</span><span class=o>.</span><span class=n>to_numpy</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>fish_target</span> <span class=o>=</span> <span class=n>fish</span><span class=p>[</span><span class=s1>&#39;Species&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>to_numpy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>train_input</span><span class=p>,</span> <span class=n>test_input</span><span class=p>,</span> <span class=n>train_target</span><span class=p>,</span> <span class=n>test_target</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>fish_input</span><span class=p>,</span> <span class=n>fish_target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 훈련 세트에서 학습한 통계 값으로 테스트 세트도 변환 해야함</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>ss</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>ss</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_input</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>train_scaled</span> <span class=o>=</span> <span class=n>ss</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>train_input</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>test_scaled</span> <span class=o>=</span> <span class=n>ss</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_input</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>SGDClassifier</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 10회 반복(10 에포크)</span>
</span></span><span class=line><span class=cl><span class=n>sc</span> <span class=o>=</span> <span class=n>SGDClassifier</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;log_loss&#39;</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sc</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_scaled</span><span class=p>,</span> <span class=n>train_target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>sc</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>train_scaled</span><span class=p>,</span> <span class=n>train_target</span><span class=p>))</span> <span class=c1># 0.773109243697479</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>sc</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>test_scaled</span><span class=p>,</span> <span class=n>test_target</span><span class=p>))</span> <span class=c1># 0.775</span>
</span></span></code></pre></td></tr></table></div></div><p>위의 값에서 10에포크를 하니 0.77 정도의 정확도가 나왔는데 50번을 돌리면</p><p>0.8739495798319328
0.8</p><p>이 정도의 정확도가 나옵니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>sc</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>(</span><span class=n>train_scaled</span><span class=p>,</span> <span class=n>train_target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>sc</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>train_scaled</span><span class=p>,</span> <span class=n>train_target</span><span class=p>))</span> <span class=c1># 0.7983193277310925</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>sc</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>test_scaled</span><span class=p>,</span> <span class=n>test_target</span><span class=p>))</span> <span class=c1># 0.775</span>
</span></span></code></pre></td></tr></table></div></div><p><code>partial_fit</code> 함수를 사용하여 추가로 훈련하니 조금 더 정확도가 오릅니다.</p><h4 id=에포크와-과대과소적합>에포크와 과대/과소적합<a hidden class=anchor aria-hidden=true href=#에포크와-과대과소적합>#</a></h4><p>에포크를 진행하면 훈련 세트 점수를 오르지만 어느 순간 테스트 세트 점수가 감소하기 시작합니다.<br>위 지점이 과대적합되기 시작하는 곳입니다. 과대적합이 시작하기 전에 훈련을 멈추는 것을 <strong>조기 종료</strong> 라고 합니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sc</span> <span class=o>=</span> <span class=n>SGDClassifier</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;log_loss&#39;</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>train_score</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>test_score</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>classes</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>train_target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 반복하면서 훈련 및 테스트 세트에 점수 기록</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>300</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>sc</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>(</span><span class=n>train_scaled</span><span class=p>,</span> <span class=n>train_target</span><span class=p>,</span> <span class=n>classes</span><span class=o>=</span><span class=n>classes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>train_score</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>sc</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>train_scaled</span><span class=p>,</span> <span class=n>train_target</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>test_score</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>sc</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>test_scaled</span><span class=p>,</span> <span class=n>test_target</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>train_score</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>test_score</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;epoch&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;accuracy&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p><img alt=image loading=lazy src=/posts/books/hg-mldl/week-3/images/image-03.webp></p><p>위 모델은 100에포크 정도가 적절한 반복 횟수로 보입니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># tol 매개변수가 None이면 100만큼 무조건 반복</span>
</span></span><span class=line><span class=cl><span class=n>sc</span> <span class=o>=</span> <span class=n>SGDClassifier</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;log_loss&#39;</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>tol</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sc</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_scaled</span><span class=p>,</span> <span class=n>train_target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>sc</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>train_scaled</span><span class=p>,</span> <span class=n>train_target</span><span class=p>))</span> <span class=c1># 0.957983193277311</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>sc</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>test_scaled</span><span class=p>,</span> <span class=n>test_target</span><span class=p>))</span> <span class=c1># 0.925</span>
</span></span></code></pre></td></tr></table></div></div><p>SGDClassifier는 tol 매개변수가 없으면 일정 에포크 동안 성능이 향상되지 않으면 더 훈련하지 않고 자동으로 멈춥니다.</p><p>그렇지만 tol을 지정하지 않고 학습하는 경우 에포크 만큼 돌지 않기 때문에 속도가 빠르지만 정확도가 더 떨어지는(0.85 쯤) 현상이 있습니다.</p><p>힌지 손실을 사용하여 훈련할 수 있습니다. 힌지 손실은 서포트 벡터 머신이라 불리며 또 다른 머신러닝 알고리즘을 위한 손실 함수입니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>sc</span> <span class=o>=</span> <span class=n>SGDClassifier</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;hinge&#39;</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>tol</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sc</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_scaled</span><span class=p>,</span> <span class=n>train_target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>sc</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>train_scaled</span><span class=p>,</span> <span class=n>train_target</span><span class=p>))</span> <span class=c1># 0.9495798319327731</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>sc</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>test_scaled</span><span class=p>,</span> <span class=n>test_target</span><span class=p>))</span> <span class=c1># 0.925</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=점진적-학습을-위한-확률적-경사-하강법>점진적 학습을 위한 확률적 경사 하강법<a hidden class=anchor aria-hidden=true href=#점진적-학습을-위한-확률적-경사-하강법>#</a></h4><p>지금까지 회귀와 분류에 널리 사용되는 알고리즘을 배웠습니다.</p><p>최근접 이웃, 선형 회귀, 릿지, 라쏘, 로지스틱 회귀, 확률적 경사 하강법 등..</p><p>위 알고리즘들은 실전에서 널리 사용되는 뛰어난 기법이지만 최고는 아니랍니다? 응..? 그럼 왜..</p><p>다음 장에서 신경망 알고리즘을 제외하고 머신러닝에서 가장 뛰어난 성능을 내는 알고리즘을 학습합니다.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://haservi.github.io/posts/books/hg-mldl/week-2/><span class=title>다음 페이지 »</span><br><span>[혼공머신] 2주차 학습 내용 정리</span></a></nav></footer><section class=comments><script>loadComment();function loadComment(){document.body.className.includes("dark")?theme="photon-dark":theme="boxy-light";let e=document.createElement("script");e.src="https://utteranc.es/client.js",e.setAttribute("repo","haservi/haservi.github.io"),e.setAttribute("issue-term","pathname"),e.setAttribute("theme",theme),e.setAttribute("crossorigin","anonymous"),e.setAttribute("async",""),document.querySelector("section.comments").innerHTML="",document.querySelector("section.comments").appendChild(e)}</script></section></article></main><footer class=footer><span>&copy; 2025 <a href=https://haservi.github.io/>Halog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin=anonymous referrerpolicy=no-referrer></script><script>const images=Array.from(document.querySelectorAll(".post-content img"));images.forEach(e=>{mediumZoom(e,{margin:0,background:"#1d1e20",scrollOffset:40,container:null,template:null})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark")),loadComment()})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="복사";function s(){t.innerHTML="복사 완료!",setTimeout(()=>{t.innerHTML="복사"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>