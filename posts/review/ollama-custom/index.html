<!doctype html><html lang=ko dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ollama LLM Model 사용하기 | Halog</title><meta name=keywords content=","><meta name=description content="Ollama를 이용해 다양한 LLM Model 구동하기"><meta name=author content><link rel=canonical href=https://haservi.github.io/posts/review/ollama-custom/><meta name=google-site-verification content="G-MXZP81P04W"><link rel=stylesheet as=style crossorigin href=https://cdn.jsdelivr.net/gh/orioncactus/pretendard@v1.3.6/dist/web/static/pretendard.css><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://haservi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://haservi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://haservi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://haservi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://haservi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1400973749140762" crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RP5NDCM8J9"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-RP5NDCM8J9",{anonymize_ip:!1})}</script><meta property="og:title" content="Ollama LLM Model 사용하기"><meta property="og:description" content="Ollama를 이용해 다양한 LLM Model 구동하기"><meta property="og:type" content="article"><meta property="og:url" content="https://haservi.github.io/posts/review/ollama-custom/"><meta property="og:image" content="https://haservi.github.io/posts/review/ollama-custom/images/cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-10T22:41:29+09:00"><meta property="article:modified_time" content="2024-06-10T22:41:29+09:00"><meta property="og:site_name" content="Halog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://haservi.github.io/posts/review/ollama-custom/images/cover.png"><meta name=twitter:title content="Ollama LLM Model 사용하기"><meta name=twitter:description content="Ollama를 이용해 다양한 LLM Model 구동하기"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://haservi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Ollama LLM Model 사용하기","item":"https://haservi.github.io/posts/review/ollama-custom/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ollama LLM Model 사용하기","name":"Ollama LLM Model 사용하기","description":"Ollama를 이용해 다양한 LLM Model 구동하기","keywords":["",""],"articleBody":"평소 AI에 관심만 있던 상황에서 최근 우연치 않게 야놀자 회사에서 LLM(Large Language Models) 관련 논문 이 LLM 우수 모델로 수상됐다는 기사 를 보고 급 관심이 가서 LLM을 어떻게 쓰는지 찾아봤습니다.\n사실 논문을 봐도 무슨 내용인지 모르겠더라구요.\n대략적으로 영어 중심 언어 모델을 다른 언어(한국어)로 확장하고 새로운 언어 토큰을 효과적으로 통합하는 방법이라 합니다. 이런 것들을 보면 정말 대단하신 분들이 많은 것 같고 나는 코딩하는 김아무개가 아닐까..\nLLM 관련하여 계속 찾다 보니 Ollama가 학습한 LLM 모델을 실행시켜 준다는 것을 알고 해당 방법으로 로컬에서 LLM 모델을 사용하는 방법에 대해 포스팅 해보겠습니다.\nOllama 란? Ollama는 다양한 오픈소스 대형 언어 모델(LLM)을 로컬 컴퓨터에서 쉽게 실행할 수 있도록 도와주는 도구입니다.\nLlama3, Mistral, codegemma등 다양한 오픈소스 LLM을 지원하며, 모델 가중치, 설정, 데이터셋을 하나의 패키지로 묶어 Modelfile로 관리할 수 있습니다.\nOllama 설치 우선 Ollama를 공식 사이트 에서 다운로드 받습니다.\n해당 글은 Window를 기준으로 하기에 Window 파일로 다운로드 받습니다.\nModels 항목에는 여러 유명 LLM 모델들이 있으니 관심이 가는 모델을 다운로드 받을 수 있습니다.\n모델 파일 D드라이브로 이동 방법 C드라이브 용량이 부족한 경우 D드라이브에 model을 저장할 수 있습니다.\n관련 이슈 를 참조해보면 환경 변수를 OLLAMA_MODELS로 저장하고 경로를 추가해주면 Ollama를 통해 모델 다운로드 시 해당 경로에 저장되는 것을 확인 할 수 있습니다.\nllama3 사용 방법 사이트 에서 제공하는 model을 이용하면 쉽게 이용이 가능합니다. 공식 사이트에서 Models 항목에 여러가지의 models이 있습니다.\n그 중 llama3가 현재 가장 인기 있는 모델 중 하나입니다. Ollama가 실행된 상태에서 터미널에 아래와 같이 입력하면 설치 후 다시 입력하면 실행 할 수 있습니다.\n1 ollama run llama3:8b 70b도 있지만 일반적인 노트북 또는 컴퓨터로는 8b정도를 돌리는 것도 생각보다 어렵습니다. 8b면 80억개의 파라미터니까 이 정도면 충분하지 않을까도 생각됩니다.\n위처럼 입력해주면 로컬 환경에서도 AI를 이용할 수 있습니다..!\n설치된 모델을 확인하고 싶은 경우 아래와 같이 입력하면 됩니다.\n1 ollama list 다른 모델 적용 방법 알림\nHugging Face에는 다양한 모델이 있지만, Ollama를 이용한 경우 거의 대부분 대화형 AI 모델만 이용할 수 있습니다.\n뭔가.. 대답도 잘하고 많이 알고 있는거 같은데 영어로 대답하니 정감이 가지 않더군요.. 찾아보니 한국어 전용 LLM이 있었습니다..!\n친절하게 .gguf 파일(링크 )도 만들어주셔서 다운로드 받아서 model을 만들어 보겠습니다.\nREADME 를 보면 파이썬으로 할 수 있는 것 같은데.. 파이썬 설치 버전 문제랑 TensorFlow??? cuda???? Flax????? 입문자가 알아야 할게 많은 것 같습니다.\ngguf 파일을 Ollama에서 동작 시킬 수 있기 때문에 아래와 같이 gguf 파일만 다운로드 받습니다.\nModelfile을 생성하는 방법은 공식 문서 에서도 볼 수 있습니다.\n그런데 이것도 모델마다 모델의 Template이 다른 것 같네요. Bllossom은 llama3를 base로 한 것 같으니 llama3 Modelfile의 template을 참조하여 작성하겠습니다. 우선 아래와 같이 설치된 llama3 모델 파일 정보를 확인합니다.\n1 ollama show --modelfile llama3 위와 같이 입력하면 llama3의 Modelfile의 정보는 아래와 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Modelfile generated by \"ollama show\" # To build a new Modelfile based on this, replace FROM with: # FROM llama3:latest FROM D:\\ProgramFiles\\ollama_model\\blobs\\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa TEMPLATE \"{{ if .System }}\u003c|start_header_id|\u003esystem\u003c|end_header_id|\u003e {{ .System }}\u003c|eot_id|\u003e{{ end }}{{ if .Prompt }}\u003c|start_header_id|\u003euser\u003c|end_header_id|\u003e {{ .Prompt }}\u003c|eot_id|\u003e{{ end }}\u003c|start_header_id|\u003eassistant\u003c|end_header_id|\u003e {{ .Response }}\u003c|eot_id|\u003e\" PARAMETER num_keep 24 PARAMETER stop \u003c|start_header_id|\u003e PARAMETER stop \u003c|end_header_id|\u003e PARAMETER stop \u003c|eot_id|\u003e LICENSE \"META LLAMA 3 COMMUNITY LICENSE AGREEMENT ... 라이센스 정보... 일단 보니까 go 느낌인거 같네요. 다운로드 받은 gguf 파일과 Modelfile의 파일명으로 아래의 정보를 추가합니다.\nFROM 절에는 gguf 다운로드 받은 파일의 절대 경로를 입력해주세요.\n한 곳에 두고 아래의 명령어를 입력하면 ollama에 모델을 학습할 수 있습니다.\n제 경우 Modelfile은 아래와 같이 입력했습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 FROM D:\\Reference\\llm_model\\llama3-korean\\llama-3-Korean-Bllossom-8B-Q4_K_M.gguf TEMPLATE \"\"\" {{ if .System }}\u003c|start_header_id|\u003esystem\u003c|end_header_id|\u003e {{ .System }}\u003c|eot_id|\u003e{{ end }}{{ if .Prompt }}\u003c|start_header_id|\u003euser\u003c|end_header_id|\u003e {{ .Prompt }}\u003c|eot_id|\u003e{{ end }}\u003c|start_header_id|\u003eassistant\u003c|end_header_id|\u003e {{ .Response }}\u003c|eot_id|\u003e \"\"\" # 모델 온도(높을수록 창의적으로 대답함 기본 0.8) PARAMETER temperature 0.1 # 텍스트 예상 토큰 수(기본 128, -1 = 무한 생성) PARAMETER num_predict 3000 PARAMETER stop \u003c|start_header_id|\u003e PARAMETER stop \u003c|end_header_id|\u003e PARAMETER stop \u003c|eot_id|\u003e 그리고 아래와 같이 명령어를 이용해 모델을 생성하면 됩니다.\n1 ollama create ko-llama3 -f Modelfile 그리고 실행을 하면 사용할 수 있습니다..!\n1 ollama run ko-llama3 처음에는 PARAMETER의 temperature를 1로 주고 모델을 생성하니 뭐랄까.. 조금 이상하게 설명을 한 것 같습니다.\n이후 답변이 마음에 안들어서 지적하니까 GPU가 부족한건지 그 뒤로 한참동안 답변이 없더라구요.. 😂\n다시 temperature를 0.1로 변경하여 모델을 만들어서 실행해보니 아래와 같이 조금은 맞게 답변을 해주는 것 같긴 했습니다.\n나무위키에서 찾아본 손흥민 선수의 정보와 다르긴 하지만 그래도 대략적으로 맞는 것을 보면 또 신기하기도 하고..\nJava 코딩도 대략 맞는 것 같네요.\nLLM에서 GGUF(Georgi Gerganov Unified Format) 파일 구조 및 어떻게 만드는지도 궁금하긴 하지만 이 정도만 알아도 사용하는데 문제는 없을 것이라 생각됩니다..!\n이것을 응용해보면 Hugging Face 사이트 에서 gguf와 관련된 모델을 찾아서 이용할 수 있습니다.\n마치며 위처럼 Hugging Face에서 모델 항목에서 다운로드 수로 정렬을 해보면 다양한 모델들이 많이 있습니다. 결국 제대로 이용하고 싶다면 python과 공식 문서 를 보면서 공부해야 할 것 같아요.\n평소에 Web Backend와 Frontend, Devops 위주로 보다 AI 쪽을 찍먹해보니.. 뭔가 다른 세상 같기도 신기하면서도 알아야 할게 많은 것 같네요.\nAI Roadmap(링크 )을 봐도 꽤 낯설긴 합니다.\n우선 다시 Backend와 Devops에 집중해야겠습니다. :D..\n","wordCount":"791","inLanguage":"ko","image":"https://haservi.github.io/posts/review/ollama-custom/images/cover.png","datePublished":"2024-06-10T22:41:29+09:00","dateModified":"2024-06-10T22:41:29+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://haservi.github.io/posts/review/ollama-custom/"},"publisher":{"@type":"Organization","name":"Halog","logo":{"@type":"ImageObject","url":"https://haservi.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://haservi.github.io/ accesskey=h title="Halog (Alt + H)">Halog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://haservi.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://haservi.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://haservi.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://haservi.github.io/>홈</a>&nbsp;»&nbsp;<a href=https://haservi.github.io/posts/>Posts</a></div><h1 class=post-title>Ollama LLM Model 사용하기</h1><div class=post-meta><span title='2024-06-10 22:41:29 +0900 +0900'>6월 10, 2024</span>&nbsp;·&nbsp;4 분</div></header><figure class=entry-cover><img loading=lazy srcset="https://haservi.github.io/posts/review/ollama-custom/images/cover_hu686053ab738bb3df0c3f0997d1f7ef81_170584_360x0_resize_box_3.png 360w ,https://haservi.github.io/posts/review/ollama-custom/images/cover_hu686053ab738bb3df0c3f0997d1f7ef81_170584_480x0_resize_box_3.png 480w ,https://haservi.github.io/posts/review/ollama-custom/images/cover_hu686053ab738bb3df0c3f0997d1f7ef81_170584_720x0_resize_box_3.png 720w ,https://haservi.github.io/posts/review/ollama-custom/images/cover.png 860w" sizes="(min-width: 768px) 720px, 100vw" src=https://haservi.github.io/posts/review/ollama-custom/images/cover.png alt="alt text" width=860 height=484><p><text></p></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>목차</span></summary><div class=inner><ul><li><a href=#ollama-%eb%9e%80 aria-label="Ollama 란?">Ollama 란?</a></li><li><a href=#ollama-%ec%84%a4%ec%b9%98 aria-label="Ollama 설치">Ollama 설치</a></li><li><a href=#%eb%aa%a8%eb%8d%b8-%ed%8c%8c%ec%9d%bc-d%eb%93%9c%eb%9d%bc%ec%9d%b4%eb%b8%8c%eb%a1%9c-%ec%9d%b4%eb%8f%99-%eb%b0%a9%eb%b2%95 aria-label="모델 파일 D드라이브로 이동 방법">모델 파일 D드라이브로 이동 방법</a></li><li><a href=#llama3-%ec%82%ac%ec%9a%a9-%eb%b0%a9%eb%b2%95 aria-label="llama3 사용 방법">llama3 사용 방법</a></li><li><a href=#%eb%8b%a4%eb%a5%b8-%eb%aa%a8%eb%8d%b8-%ec%a0%81%ec%9a%a9-%eb%b0%a9%eb%b2%95 aria-label="다른 모델 적용 방법">다른 모델 적용 방법</a></li><li><a href=#%eb%a7%88%ec%b9%98%eb%a9%b0 aria-label=마치며>마치며</a></li></ul></div></details></div><div class=post-content><p>평소 AI에 관심만 있던 상황에서 최근 우연치 않게 야놀자 회사에서 LLM(Large Language Models) 관련 <a href=https://arxiv.org/pdf/2402.14714 target=_blank>논문</a>
이
LLM 우수 모델로 수상됐다는 <a href="https://www.aitimes.com/news/articleView.html?idxno=158608" target=_blank>기사</a>
를 보고 급 관심이 가서 LLM을 어떻게 쓰는지 찾아봤습니다.</p><p>사실 논문을 봐도 무슨 내용인지 모르겠더라구요.</p><p>대략적으로 영어 중심 언어 모델을 다른 언어(한국어)로 확장하고 새로운 언어 토큰을 효과적으로 통합하는 방법이라 합니다. 이런 것들을 보면 정말 대단하신 분들이 많은 것 같고 나는 코딩하는 김아무개가 아닐까..</p><p>LLM 관련하여 계속 찾다 보니 Ollama가 학습한 LLM 모델을 실행시켜 준다는 것을 알고 해당 방법으로 로컬에서 LLM 모델을 사용하는 방법에 대해 포스팅 해보겠습니다.</p><h3 id=ollama-란>Ollama 란?<a hidden class=anchor aria-hidden=true href=#ollama-란>#</a></h3><p>Ollama는 다양한 오픈소스 대형 언어 모델(LLM)을 로컬 컴퓨터에서 쉽게 실행할 수 있도록 도와주는 도구입니다.</p><p>Llama3, Mistral, codegemma등 다양한 오픈소스 LLM을 지원하며, 모델 가중치, 설정, 데이터셋을 하나의 패키지로 묶어 Modelfile로 관리할 수 있습니다.</p><h3 id=ollama-설치>Ollama 설치<a hidden class=anchor aria-hidden=true href=#ollama-설치>#</a></h3><p>우선 Ollama를 공식 <a href=https://www.ollama.com/ target=_blank>사이트</a>
에서 다운로드 받습니다.</p><p>해당 글은 Window를 기준으로 하기에 Window 파일로 다운로드 받습니다.</p><p><img loading=lazy src=./images/image00.png#center alt=image></p><p><img loading=lazy src=./images/image01.png#center alt=image></p><p>Models 항목에는 여러 유명 LLM 모델들이 있으니 관심이 가는 모델을 다운로드 받을 수 있습니다.</p><p><img loading=lazy src=./images/image02.png#center alt=image></p><h3 id=모델-파일-d드라이브로-이동-방법>모델 파일 D드라이브로 이동 방법<a hidden class=anchor aria-hidden=true href=#모델-파일-d드라이브로-이동-방법>#</a></h3><p>C드라이브 용량이 부족한 경우 D드라이브에 model을 저장할 수 있습니다.</p><p>관련 <a href=https://github.com/ollama/ollama/issues/2546 target=_blank>이슈</a>
를 참조해보면 환경 변수를 <code>OLLAMA_MODELS</code>로 저장하고
경로를 추가해주면 Ollama를 통해 모델 다운로드 시 해당 경로에 저장되는 것을 확인 할 수 있습니다.</p><p><img loading=lazy src=./images/image03.png#center alt=image></p><h3 id=llama3-사용-방법>llama3 사용 방법<a hidden class=anchor aria-hidden=true href=#llama3-사용-방법>#</a></h3><p><a href=https://www.ollama.com/ target=_blank>사이트</a>
에서 제공하는 model을 이용하면 쉽게 이용이 가능합니다. 공식 사이트에서 Models 항목에 여러가지의 models이 있습니다.</p><p>그 중 llama3가 현재 가장 인기 있는 모델 중 하나입니다. Ollama가 실행된 상태에서 터미널에 아래와 같이 입력하면 설치 후 다시 입력하면 실행 할 수 있습니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama run llama3:8b
</span></span></code></pre></td></tr></table></div></div><p>70b도 있지만 일반적인 노트북 또는 컴퓨터로는 8b정도를 돌리는 것도 생각보다 어렵습니다. 8b면 80억개의 파라미터니까 이 정도면 충분하지 않을까도 생각됩니다.</p><p>위처럼 입력해주면 로컬 환경에서도 AI를 이용할 수 있습니다..!</p><p><img loading=lazy src=./images/image05.png#center alt=image></p><p>설치된 모델을 확인하고 싶은 경우 아래와 같이 입력하면 됩니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama list
</span></span></code></pre></td></tr></table></div></div><h3 id=다른-모델-적용-방법>다른 모델 적용 방법<a hidden class=anchor aria-hidden=true href=#다른-모델-적용-방법>#</a></h3><style type=text/css>.notice{--root-color:#000;--root-background:#e7f2fa;--title-color:#fff;--title-background:#6ab0de;--warning-title:rgba(217, 83, 79, 0.9);--warning-content:#fae2e2;--info-title:#f0b37e;--info-content:#fff2db;--note-title:#6ab0de;--note-content:#e7f2fa;--tip-title:rgba(92, 184, 92, 0.8);--tip-content:#e6f9e6}body.dark .notice{--root-color:#fff;--root-background:#e7f2fa;--title-color:#fff;--title-background:#6ab0de;--warning-title:rgba(130, 49, 47, 0.9);--warning-content:#341312;--info-title:#906B4B;--info-content:#392A1E;--note-title:#3f6985;--note-content:#192A35;--tip-title:rgba(46, 92, 46, 0.8);--tip-content:#122412}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative}</style><div><svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg"><symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379.0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628.0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628.0l-22.627 22.627c-6.248 6.248-6.248 16.379.0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"/></symbol><symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet"><path d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937.0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154.0l239.94 416.028zM288 354c-25.405.0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346 7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373.0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884.0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196.0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627.0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627.0 12 5.373 12 12v1e2h12c6.627.0 12 5.373 12 12v24z"/></symbol></svg></div><div class="notice info"><p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#info-notice"/></svg></span>알림</p><p>Hugging Face에는 다양한 모델이 있지만, Ollama를 이용한 경우
거의 대부분 대화형 AI 모델만 이용할 수 있습니다.</p></div><p>뭔가.. 대답도 잘하고 많이 알고 있는거 같은데 영어로 대답하니 정감이 가지 않더군요.. 찾아보니 한국어 전용 LLM이 있었습니다..!</p><p>친절하게 .gguf 파일(<a href=https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M target=_blank>링크</a>
)도 만들어주셔서 다운로드 받아서 model을 만들어 보겠습니다.</p><p><a href=https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M/blob/main/README.md target=_blank>README</a>
를 보면 파이썬으로
할 수 있는 것 같은데.. 파이썬 설치 버전 문제랑 TensorFlow??? cuda???? Flax????? 입문자가 알아야 할게 많은 것 같습니다.</p><p>gguf 파일을 Ollama에서 동작 시킬 수 있기 때문에 아래와 같이 gguf 파일만 다운로드 받습니다.</p><p><img loading=lazy src=./images/image06.png#center alt=image></p><p>Modelfile을 생성하는 방법은 <a href=https://github.com/ollama/ollama/blob/main/docs/modelfile.md target=_blank>공식 문서</a>
에서도 볼 수 있습니다.</p><p>그런데 이것도 모델마다 모델의 Template이 다른 것 같네요. Bllossom은 llama3를 base로 한 것 같으니 llama3 Modelfile의 template을
참조하여 작성하겠습니다. 우선 아래와 같이 설치된 llama3 모델 파일 정보를 확인합니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama show --modelfile llama3
</span></span></code></pre></td></tr></table></div></div><p>위와 같이 입력하면 llama3의 Modelfile의 정보는 아래와 같습니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Modelfile generated by &#34;ollama show&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># To build a new Modelfile based on this, replace FROM with:</span>
</span></span><span class=line><span class=cl><span class=c1># FROM llama3:latest</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>FROM D:<span class=se>\P</span>rogramFiles<span class=se>\o</span>llama_model<span class=se>\b</span>lobs<span class=se>\s</span>ha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa
</span></span><span class=line><span class=cl>TEMPLATE <span class=s2>&#34;{{ if .System }}&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>{{ .System }}&lt;|eot_id|&gt;{{ end }}{{ if .Prompt }}&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>{{ .Prompt }}&lt;|eot_id|&gt;{{ end }}&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>{{ .Response }}&lt;|eot_id|&gt;&#34;</span>
</span></span><span class=line><span class=cl>PARAMETER num_keep <span class=m>24</span>
</span></span><span class=line><span class=cl>PARAMETER stop &lt;<span class=p>|</span>start_header_id<span class=p>|</span>&gt;
</span></span><span class=line><span class=cl>PARAMETER stop &lt;<span class=p>|</span>end_header_id<span class=p>|</span>&gt;
</span></span><span class=line><span class=cl>PARAMETER stop &lt;<span class=p>|</span>eot_id<span class=p>|</span>&gt;
</span></span><span class=line><span class=cl>LICENSE <span class=s2>&#34;META LLAMA 3 COMMUNITY LICENSE AGREEMENT
</span></span></span><span class=line><span class=cl><span class=s2>... 라이센스 정보...
</span></span></span></code></pre></td></tr></table></div></div><p>일단 보니까 go 느낌인거 같네요. 다운로드 받은 gguf 파일과 Modelfile의 <code>파일명</code>으로 아래의 정보를 추가합니다.</p><p>FROM 절에는 gguf 다운로드 받은 파일의 절대 경로를 입력해주세요.</p><p>한 곳에 두고 아래의 명령어를 입력하면 ollama에 모델을 학습할 수 있습니다.</p><p>제 경우 Modelfile은 아래와 같이 입력했습니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>FROM D:<span class=se>\R</span>eference<span class=se>\l</span>lm_model<span class=se>\l</span>lama3-korean<span class=se>\l</span>lama-3-Korean-Bllossom-8B-Q4_K_M.gguf
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>TEMPLATE <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>{{ if .System }}&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>{{ .System }}&lt;|eot_id|&gt;{{ end }}{{ if .Prompt }}&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>{{ .Prompt }}&lt;|eot_id|&gt;{{ end }}&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>{{ .Response }}&lt;|eot_id|&gt;
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 모델 온도(높을수록 창의적으로 대답함 기본 0.8)</span>
</span></span><span class=line><span class=cl>PARAMETER temperature 0.1
</span></span><span class=line><span class=cl><span class=c1># 텍스트 예상 토큰 수(기본 128, -1 = 무한 생성)</span>
</span></span><span class=line><span class=cl>PARAMETER num_predict <span class=m>3000</span>
</span></span><span class=line><span class=cl>PARAMETER stop &lt;<span class=p>|</span>start_header_id<span class=p>|</span>&gt;
</span></span><span class=line><span class=cl>PARAMETER stop &lt;<span class=p>|</span>end_header_id<span class=p>|</span>&gt;
</span></span><span class=line><span class=cl>PARAMETER stop &lt;<span class=p>|</span>eot_id<span class=p>|</span>&gt;
</span></span></code></pre></td></tr></table></div></div><p>그리고 아래와 같이 명령어를 이용해 모델을 생성하면 됩니다.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama create ko-llama3 -f Modelfile
</span></span></code></pre></td></tr></table></div></div><p>그리고 실행을 하면 사용할 수 있습니다..!</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama run ko-llama3
</span></span></code></pre></td></tr></table></div></div><p>처음에는 PARAMETER의 temperature를 1로 주고 모델을 생성하니 뭐랄까.. 조금 이상하게 설명을 한 것 같습니다.</p><p><img loading=lazy src=./images/image08.png#center alt=image>
<img loading=lazy src=./images/image09.png#center alt=image></p><p>이후 답변이 마음에 안들어서 지적하니까 GPU가 부족한건지 그 뒤로 한참동안 답변이 없더라구요.. 😂</p><p>다시 temperature를 0.1로 변경하여 모델을 만들어서 실행해보니 아래와 같이 조금은 맞게 답변을 해주는 것 같긴 했습니다.</p><p><img loading=lazy src=./images/image10.png#center alt=image></p><p>나무위키에서 찾아본 손흥민 선수의 정보와 다르긴 하지만 그래도 대략적으로 맞는 것을 보면 또 신기하기도 하고..</p><p>Java 코딩도 대략 맞는 것 같네요.</p><p>LLM에서 GGUF(Georgi Gerganov Unified Format) 파일 구조 및 어떻게 만드는지도 궁금하긴 하지만 이 정도만 알아도 사용하는데 문제는 없을 것이라 생각됩니다..!</p><p>이것을 응용해보면 Hugging Face <a href="https://huggingface.co/models?sort=downloads&search=gguf" target=_blank>사이트</a>
에서 gguf와 관련된 모델을 찾아서 이용할 수 있습니다.</p><h3 id=마치며>마치며<a hidden class=anchor aria-hidden=true href=#마치며>#</a></h3><p><img loading=lazy src=./images/image12.png#center alt=image></p><p>위처럼 Hugging Face에서 모델 항목에서 다운로드 수로 정렬을 해보면 다양한 모델들이 많이 있습니다. 결국 제대로 이용하고 싶다면 python과 <a href=https://huggingface.co/docs/transformers/v4.41.3/ko/installation target=_blank>공식 문서</a>
를
보면서 공부해야 할 것 같아요.</p><p>평소에 Web Backend와 Frontend, Devops 위주로 보다 AI 쪽을 찍먹해보니.. 뭔가 다른 세상 같기도 신기하면서도 알아야 할게 많은 것 같네요.</p><p>AI Roadmap(<a href=https://roadmap.sh/ai-data-scientist target=_blank>링크</a>
)을 봐도 꽤 낯설긴 합니다.</p><p>우선 다시 Backend와 Devops에 집중해야겠습니다. :D..</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://haservi.github.io/posts/books/programmers-java/><span class=title>« 이전 페이지</span><br><span>[도서] 취업과 이직을 위한 프로그래머스 코딩 테스트 문제 풀이 전략 리뷰</span></a>
<a class=next href=https://haservi.github.io/posts/tools/how-to-raycast/><span class=title>다음 페이지 »</span><br><span>Raycast를 이용하여 업무 효율성 높이기</span></a></nav></footer><section class=comments><script>loadComment();function loadComment(){document.body.className.includes("dark")?theme="photon-dark":theme="boxy-light";let e=document.createElement("script");e.src="https://utteranc.es/client.js",e.setAttribute("repo","haservi/haservi.github.io"),e.setAttribute("issue-term","pathname"),e.setAttribute("theme",theme),e.setAttribute("crossorigin","anonymous"),e.setAttribute("async",""),document.querySelector("section.comments").innerHTML="",document.querySelector("section.comments").appendChild(e)}</script></section></article></main><footer class=footer><span>&copy; 2025 <a href=https://haservi.github.io/>Halog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin=anonymous referrerpolicy=no-referrer></script>
<script>const images=Array.from(document.querySelectorAll(".post-content img"));images.forEach(e=>{mediumZoom(e,{margin:0,background:"#1d1e20",scrollOffset:40,container:null,template:null})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark")),loadComment()})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="복사";function s(){t.innerHTML="복사 완료!",setTimeout(()=>{t.innerHTML="복사"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>